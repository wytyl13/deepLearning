# -*- coding: utf-8 -*-
#*****************************************************************
#   Copyright (C) 2022 IEucd Inc. All rights reserved.
#   
#   @Author: weiyutao
#   @Created Time : 2022/8/14 16:20:20
#   @File Name : CNNBasicDefine.py
#   @Description :下来我们来实现一个完整的标准的卷积神经网络的定义，其中包含了network类的定义和forward方法的实现 
#
#*****************************************************************
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import torchvision
import torchvision.transforms as transforms

torch.set_printoptions(linewidth = 120)

class Network(nn.Module):
    def __init__(self):
        super().__init__()
        #实现对第一二隐藏卷积层卷积操作的定义
        self.conv1 = nn.Conv2d(in_channels = 1,out_channels = 6,kernel_size = 5)
        self.conv2 = nn.Conv2d(in_channels = 6,out_channels = 12,kernel_size = 5)

        #实现对线性层的定义，也就是狭义的前向传播，也就是使用权重矩阵对输入张量进行降维
        #注意这里的线性层是第一二隐藏卷积层和输出层的线性层，输入层不需要定义，因为输入层的输入就是输出。
        self.fc1 = nn.Linear(in_features = 12 * 4 * 4,out_features = 120)#第一隐藏线性层的输入特征值数量就等于第二隐藏卷积层的输出通道的数量
        self.fc2 = nn.Linear(in_features = 120,out_features = 60)
        self.out = nn.Linear(in_features = 60,out_features = 10)


    #上面我们已经实现了该卷积神经网络类的属性的定义，下面我们来定义下该卷积神经网络的前向传播方法forward，注意这里的方法的定义是广义的前向传播方法的定义，其中按照步骤包含了线性层-卷积层-激活函数-最大池化操作-输出作为第二层的输入
    def forward(self,t):#前向传播方法需要进行形参的传递，传递的形参就是我们的输入张量，是一个tensor对象,我们将形参t在该方法中进行了一系列操作以后会得到一个返回结果，就是我们前向传播的最终结果，此处我们都是在t的原始数据上进行的操作，这样的好处是不占用太多的内存空间，下面我们来按照每一层的顺序来定义前向传播
        
        #input layer
        t = t

        #first hidden conv layer
        t = self.conv1(t)#使用定义好的第一隐藏层的卷积操作属性，传入一个张量对象t进行卷积
        t = F.relu(t) #此处使用的是torch.nn.functional库中的relu方法来进行激活函数的求解
        t = F.max_pool2d(t,kernel_size = 2,stride = 2)#还是使用funcitonal库中的最大池化方法进行最大池化操作，传入激活函数输出的张量，定义最大池化的工具尺寸是2*2，步伐是2,到这，第一隐藏卷积层就已经完成了，可以作为第二隐藏卷积层的输入了

        #second hidden conv layer
        t = self.conv2(t)
        t = F.relu(t)
        t = F.max_pool2d(t,kernel_size = 2,stride = 2)

        #first hidden linear layer
        t = t.reshape(-1,12*4*4)
        t = self.fc1(t)
        t = F.relu(t)

        #second hidden linear layer
        t = self.fc2(t)
        t = F.relu(t)

        #output layer
        t = self.out(t)
        #t = F.sofymax(t,dim = 1)，如果是多分类，那么就还需要添加输出层的激活函数softmax
        return t
"""
这里我们需要注意下层和操作的区别，当我们进行层的操作的时候，比如卷积层的操作或者线性层的操作，我们是借助权重，张量权重或者权重矩阵进行的操作，那么上面的权重是由我们的传入的参数然后在Conv2d方法内部生成的，对应生成的每层的权重也存储在每层的对象上，比如self.conv1,它就是第一隐藏卷积层层的属性，而self.fc1他就是第一隐藏线性层的属性，相关的权重都保存在该属性中，我们可以使用点进行访问。而对应的操作比如激活操作和池化操作，他不能称之为层，因为它没有权重也不需要权重，我们只是简单的调用了下functional库中的激活或者池化操作的方法，所以这就是层和操作的区别，层包含了很多的内置东西，而操作其实就是一个方法。层其实就是一个属性或者也可以称之为一个专用的对象。
    例如我们会说，我们网络中的第二层是一个包含权重集合的卷积层，并执行三个操作：卷积操作、激活操作和最大池化操作。所以简单来讲，使用权重定义的操作就是我们所说的层，后来才有的其他的操作比如激活操作和池化操作被添加到组合中，我们称其为层。


隐藏的线性层，我们有必要重点关注下
    在我们将输入传递给第一个隐藏线性层之前，我们必须reshape或者展平我们的张量。每当我们将卷积层的输出作为输入传递给线性层时，都会出现这种情况
"""

"""
case 1:
OK，以上我们已经构建了基本的CNN神经网络结构以及前向传播方法，下面我来使用torch框架进行详细的使用
    在我们的案例中，从实际的角度来看，前向传播是将输入图像张量传递给forward方法，然后通过网络定义的步骤最后输出的是预测值
    在数据集和数据加载器的那部分，我们看到了如何从我们的训练集中访问单个样本图像张量。或者如何从我们的数据加载器中访问一批图像张量，现在我们来将批量图像好擦UN递给forward方法，以获得预测
"""

#返回正确的预测个数
def get_num_correct(preds,labels):
    return preds.argmax(dim = 1).eq(labels).sum()


if __name__ == "__main__":
    
    #test case 1:
    #首先，让我们从不进行梯度计算开始，也即我们不进行参数的更新，我们可以先选择关闭它，然后在训练开始时重新打开它
    #其次，我们将关闭pytorch的动态计算图。关闭它绝对不是必要的，但是关闭该功能确实可以减少内存消耗，因为图形将不存储在内存中。
    #torch.set_grad_enabled(False)
    
    #创建网络实例
    network = Network()
    
    #读取数据
    train_set = torchvision.datasets.FashionMNIST(
        root = './data',
        train = True,
        download = True,
        transform = transforms.Compose([transforms.ToTensor()]),
    )
    
    """
    获取单张图像并且传入神经网络


    #直接从train_set数据集中读取单张图像，因为我们并没与定义batch_size,所以默认的batch_size是1
    sample = next(iter(train_set))
    image,label = sample
    print(image.shape)#1*28*28
    
    #画取单张图像的时候将颜色通道挤压掉因为它为1
    plt.imshow(image.squeeze(),cmap = 'gray')
    plt.show()
    
    #下面我们需要做的就是将图像传入我们的网络，但是在传递之前还有一个事情需要处理，就是我们直接取出来的是3阶轴长分别为1，28，28的张量，分别表示的是颜色通道，高宽，我们支出的网络输入格式是4阶张量，在最前面加入一个批次就可以。即便我们只有一张图像，我们也应该加入批次1，我们可以使用tensor张量数据结构的unsqueeze方法，这个是解压，可以在指定的索引位置增加一个轴长为1的维度i,tensor.unsqueeze(0)
    print(image.unsqueeze(0).shape)#1*1*28*28

    #现在我们可以将得到的解压后的张量传入网络了，我们直接给network对象传入该4阶张量对象即可，不需要直接调用forward方法，内部会自动调用call方法，call方法会调用forward方法。这个我们在之前已经有所赘述

    pred = network(image.unsqueeze(0))#会输出预测，当然预测数据是二阶张量，对应到数学上就是一个向量的转置，该预测张量对应的轴长为1*10，也就是数据的类别数量
    print(pred)#输出的是二阶张量，轴长是1*10，取其中最大值索引即为预测值
    index = pred.argmax(dim = 1)#返回最大值的索引，dim = 0是竖向最大，dim = 1是横向最大索引，1*10取横向最大值索引会返回一个标量张量
    print(index,label)
    #注意，因为我们只前向传播了一次，并没有进行后向传播也就是梯度下降，所以高预测值是不准确的，因为它使用了随机初始化的权重，并且每次运行的结果都不一样，注意这点。而且需要注意的是，返回的预测张量中的每一个元素代表的是其对对应索引位置的衣服类型的预测的接近程度，最大的说明越可能是哪一个服装类型
    
    #对于预测值，我们可以找出最大值，当然我们也可以计算出其对应的概率，概率相加也为1，此处需要区分预测值和预测概率的区别，我们需要找预测值中最大的索引号来判断该图片的服装类型，而预测概率也一样，只不过是找概率最大的，我们可以使用softmax函数来求得预测概率
    print(F.softmax(pred,dim = 1))

    """
    
    #下面我们获取批次图像并且传入网络，注意批次是使用data_loader定义的。定义batch_size，然后读取对应的dataloader数据集中的数据即可。上面的示例我们读取的是一张图像，所以我们直接从原始数据集train_set中读取，这次我们先对train_set使用dataloader工具进行分批次，然后再读取批次图像，读取的方式有很多，默认的是依批次读取，可以设置为在每个批次中取出一个读取。然后是返回的数据类型，直接读取返回的是一个3阶张量对应的轴长分别为颜色通道、高和宽，而使用loader加载器读取到的是一个4阶张量，对应的轴长分别为批次也就是一个批次对应的图像数量，颜色通道和高宽。而我们在传入神经网络的时候需要的数据就是一个四阶张量，所以可以直接将loader加载器返回的张量作为形参传递进神经网络，而直接从train_set中读取的数据需要解压才能传入。解压即增加一个维度为1的轴长，对应的增加位置可以传递索引进行设置，一般都是向0索引处增加一个轴长为1的。
    data_loader = torch.utils.data.DataLoader(
        train_set,
        batch_size = 100,
    )
    
    batch = next(iter(data_loader))
    images,labels = batch
    print(images.shape,labels.shape)#对应的张量的轴长分别为10*1*28*28   10，对应的就是返回10张图像，每张图像一个标签。

    preds = network(images)#直接将批量图片的张量数据传入网络对象实例,会返回预测值

    print(preds,'\n',preds.shape)#输出的预测值是一个2阶张量，轴长分别为10*10,也就是对于传入的10章图像，我们有10个预测类，每一个数组元素都包含了对应图像的每个类别的10个预测。对应的每一个横向维度是一组预测。我们只需要找到一组预测中预测值最大的对应的索引编号即是每张图像的预测类别，对应的在数字编码中找到对应数据对应的服装类别即可得到预测的结果

    result = preds.argmax(dim = 1)
    print(result,'\n',result.shape,'\n',labels)#返回的result是一个二阶张量，轴长分别为1*10,然后分别和label去比较即可
    #由于我们在之前的模型中关闭了梯度计算，也就是我们不进行参数的迭代更新，所以这次的预测结果就是使用初始化权重矩阵以此计算的预测值。所以这样的结果不能实现预测，为了结果准确，我们需要开启梯度下井进行参数的更新其实就是加入了反向传播。
    #我们可以试着看下准确率
    correctRate = preds.argmax(dim = 1).eq(labels)
    correctSum = get_num_correct(preds,labels) 
    print(correctRate,"\n",correctSum)

    #OK,到这里我们已近可以传入一个批量的图像到神经网络并且可以得到预测值，但是并没有进行参数的更新，只进行了前向传播，这个在后续进行，下来我们先对每一层每一步骤运行后输出的张量的轴长进行深入分析。
    #首先我们给出总结的公式O = (n-f+2p)/s + 1,其中n是输入数据的轴长，f是过滤器轴长（在卷积操作中i的过滤器和最大池化操作中的过滤器），p填充的大小，1个轴长的填充p就是1，s是最大池化操作的步长。O是每一层神经网络的输出轴长。卷积操作中一般步长s是1，而在最大池化操作中一般步长是2，但是也可以自定义步长，因为步长默认就是1，1是最小步长
    #每一步骤的通道数在输入层是颜色通道的数量，在隐藏卷积层是过滤器的数量，在线性层不考虑通道，因为线性层都是2阶张量，而对应的卷积隐藏层是4阶张量。
    """
    输入层的输出也即隐藏卷积层的输入：1*1*28*28  图像批次*颜色通道*高*宽
    第一隐藏卷积层卷积操作的输出也即激活层和最大池化层的输入：因为激活层不改变张量的轴长 1*6*24*24 此处使用计算公式O = (28-5+2*0)/1+1 = 24  
    第一隐藏最大池化操作输出也即下一个隐藏卷积层的输入：1*6*12*12  O = （24-2+2*0）/2+1 = 12
    第二隐藏卷积层卷积操作的输出也即激活层和最大池化层的输入：1*12*8*8  O = （12-5+2*0）/1+1 = 8
  : 第二隐藏卷积层最大池化操作输出也即第一个线性层的输入：1*12*4*4   O = （8-2+2*0）/2+1 = 4
    注意：从线性层开始就要对张量进行挤压了，也就是说我们要在线性层要对元素进行操作，将一个图像的元素使用flatten方法挤压成一条元素
    第一隐藏线性层输出：第一线性层的输入就是将1*12*4*4挤压成1*192，所以输入是1*192，而输出则是由神经网络的神经元个数决定，本案例中shi 1*120
    第二隐藏线性层输出：1*60
    输出层：1*10  由输入数据的类别个数决定
    

    以上：对应的输入O是正向方的，而如果是长方形的输入如何计算呢？将宽度和高度分开计算即可。计算高度的O的时候都是用对应的高度的n和高度的f，而计算宽度的O的时候则分别使用宽度的O和宽度的f

    """
    
    #下来是计算损失
    loss = F.cross_entropy(preds,labels)
    print(loss.item())
    
    #接下来我们开始反向传播，从损失函数开始反向传播，损失函数就是包含的参数最全的表达式，一切反向求导都是对其求偏导开始的
    #注意，每一层的可更新参数的权重梯度的访问方式是属性层.weight.grad，其实就是该参数的求导，所有的权重张量都是放在每一层的属性conv1 conv2 fc1 fc2 out等的weight属性里的，这里包含了前向传播和反向传播所有的权重张量.那么在之前的深度学习中我们可以发现，求导根本不会改变张量的轴长，所以我们不需要深究反向求导对参数张量的影响，因为不会影响
    print(network.conv1.weight.grad)#访问network实例化对象下的conv1层也就时第一隐藏卷积层下的weight属性下的grad属性，也就时反向梯度.我们可以发现该属性为空，因为我们并没有使用梯度，也就时并没有进行反向传播，所以肯定没有，到这我们只进行了一次正向传播。
    #loss.requires_grad_(True)
    
    #loss.retain_grad()使用backward方法后grad还未none的解决办法之一：定义保留非叶节点的梯度参数
    loss.backward()#进行一次梯度下降
    #注意这块如果直接使用对loss使用backward方法会报错，因为我们在计算损失函数的时候该函数中默认的参数requires_grad为False,如果我们在当时也就是计算损失值的时候没有设置这个参数为True，那么在使用该loss表达式进行反向传播的时候就需要加上true，如上，或者可以在计算损失值的时候加入参数为true的定义loss = F.cross_entropy(preds,labels,requires_grad = True)
    
    #在进行了反向传播以后，我们就可以取到里面的grad参数了，也就是反向传播时候计算并且保存的反向求导也即对从损失函数J开始的对本层参数的求偏导
    #但是我们在这遇到了一个问题，就是我们在定义了requires_grad参数以后，然后再进行反向传播，还是没有保存到grad，这是为什么呢？
    #首先我们可以看下我们的对某一个张量做反向传播，一般是对最后一个张量做反向传播，也就是对损失函数。对某一个张量做反向传播。然后默认会只保留叶节点的梯度grad，所以如果想要访问非叶节点的梯度就需要进行设置。
    
    #由于使用了backward方法以后输出grad梯度还为n，所以我们有必要对该中情况进行总结
        #首先，列出几个满足计算tensor梯度的条件
            #1 tensor的类型为叶子节点   tensor.is_leaf
            #2 requires_grad = true  定义办法是loss.requires_grad(True),也就是对谁做backward，就对谁设置，本案例中是对loss反向传播
            #3 依赖该tensor的所有tensor的requires_grad都为true
        #但是根据经验，还有一个大招可以关闭梯度，就是torch.set_grad_enabled = False,这个一旦关闭，以上的全部设置无用。而且这个设置一般要在正向传播之前就要设置好，否则无用。
        #经过分析，本文打印出grad为none的原因就是设置了false，并且企图在正向传播之后将其设置为true才导致的输出grad为none，所以引以为戒。
    print(loss.is_leaf)#后续我们还需要知道什么是叶节点什么是非叶节点。
    print(network.conv1.weight.grad.shape,network.conv2.weight.grad.shape,network.fc1.weight.grad.shape,network.fc2.weight.grad.shape,network.out.weight.grad.shape)
    #我们可以打印输出下每一层的权重参数中的grad也就是梯度，反向求导后的张量的轴长，后续需要对其进行深入分析

    #上面我们使用了梯度下降（也就是反向传播）来测试，下面我们可以试着使用更加高级的优化算法，我们可以使用多种优化方法，比如Adam和SGD，还有更加简答的就是随机梯度下降算法，他较梯度下降算法增加的就是对单个训练样本进行的优化，梯度下降是对所有样本进行的优化。也就是说前者是对每一个样本分别进行逐次优化，而后者是对所有样本一起进行优化。
    #我们也可以使用小批量梯度下降，也就是mini_batch，介于梯度下降和随机梯度下降之间。mini_batch的操作是先将训练集数据打乱，然后从中依次取出对应批次的图像进行优化，优化速度介于两者之间，效果也是介于两者之间。
    #我们也可以使用包含动量的地图下降。由于mini_batch只看到了一个子集的参数更新，更新的方向有一定的差异，所以会在不断的震荡中走向收敛，我们可以使用动量减少这些震荡。动量考虑了过去的梯度以平滑更新。我们会将以前的梯度方向存储在变量中。也就是前面梯度的指数加权平均。而这也会有一个缺陷，当天数接近0的时候，如果单纯的使用指数加权平均值会出现很大的偏差，所以为了解决这个问题我们考虑使用偏差修正，这就是一个完整的momentnum优化算法，他可以减缓梯度收敛时候的震荡，从而加快优化速率
    #而到这我们有必要说一下，以上提到的优化算法，都是在梯度下降算法的基础上进行的，也就是说以上优化算法都会操作梯度下降，也就是反向传播。只不过它增加了优化效率，比如动量梯度下降算法，它是在求导的基础上考虑了上一期的梯度，从而导致了在梯度收敛的时候不是那么震荡，表现的更加平滑。这就是指数加权平均数在梯度下降中的应用。我们可以在任何问题上使用指数加权平均数。本来我们在迭代更新的时候是对梯度下降的dw和db进行的操作，这是原始的办法，比如w = W - α*dW,但是因为震荡的原因，我们对dw稍作处理，我们使用指数加权平均数vdw代替dW，每一期的vdw_t = β*vdw_t-1 + (1-β)*dw_t,其中的β是一个稍小于1的数。一般是0.99，因为β稍小于1所以当期较上期的变化变得开始平缓。其实也就是本期的vvdvw也就是本期dw的指数加权平均数,等于上一期的指数加权平均数和本期的dw以一个接近于1的β和（1-β）为权重计算得到的，所以他的值是在上一期指数加权平均数的基础上考虑的本期的dw值，所以非常平滑。w_t = w_t -aerfa*vdw_t
    #PMSpro优化算法：是在momentnum的基础上考虑了对dw的平方，sdw_t = beita*sdw_t-1 + (1- beita)*dw^2,然后在更新参数的时候和momentnum却有很大的差别考虑开根号，也就是w = w - aerfa * dw / (sdw^(1/2) * epsilon)
    #Adam：momentnum和PMSprop两种优化算法的结合
    #SGD
    #下面我们使用下torch的Adam优化算法去代替梯度下降算法，单纯的梯度下降算法最原始的就是backward，后面的所有优化算法都是在backward的基础上定义的，比如Adam，所以这两个方法是平级的
    optimizer = optim.Adam(network.parameters(),lr = 0.01)#因为包装好的算法，所以我们直接传入学习率即可，注意这个只是定义了优化模型，我们还没有进行迭代，所以optimizer仅仅是我们创建了一个关于network实例的优化器，在后续我们需要使用其进行优化
    
    print("使用初始化可学习参数计算的损失值和准确率，样本是100个\n")
    print(loss.item())#拿出损失值
    print(get_num_correct(preds,labels))#注意这个，输出的结果是判断正确的个数，然后使用它除以训练的样本个数就是正确率

    #注意因为我们并没有使用优化器进行迭代，所以这个正确率还是我们只进行了一次正向传播所得到的正确率

    optimizer.step()#该方法是使用优化器优化了一次，也就是进行了一次反向传播，注意此时网络中的参数都已经更新了一次了，也就是在使用优化器的step方法以后，已经进行了一次完整优化，注意这里说的完整优化是什么，完整优化是优化算法+优化参数，优化算法包含了后向传播和以后想传播为基础在内的所有优化算法。而一个完整的优化算法比如最普通的梯度下降算法，它包含了后向传播和参数更新，后向传播是计算dw和db，参数更新是使用学习率和偏导数对算法进行优化也就是w -= aerfa*dw.而本案例总的optimizer是一个使用Adam优化算法建立的以network实例化网络为基础的优化器，我们使用step方法就是进行了一次完整的优化。注意优化我们不要和前向传播高在一块，方便记忆。

    #在进行了一次前向传播和一次完整的优化以后，我们也可以将一次前向传播和一次优化统一称为一次完整的优化，我们的network实例化对象中的可更新参数已经不是之前的初始化值了，我们可以使用相同的图片来看下效果
    print("在使用了一次完整的优化算法以后，使用优化后的参数导入相同的图像计算的损失值和准确率\n")
    preds = network(images)#还是使用同样的图片和标签
    loss = F.cross_entropy(preds,labels)
    print(loss.item())
    print(get_num_correct(preds,labels))
    #可以看到损失值下降了，并且准确率上升了



    """
    #我们来使用完整代码总结下以上的一次完整的优化
    network = Network() 实例化对象
    train_loader = torch.utils.data.DataLoader(train_set,batch_size = 100)  定义加载起，加载数据，直接返回所有分好批次的数据，张量
    optimizer = optim.Adam(network,lr = 0.01) 定义优化器，基于优化算法和神经网络实例化对象，只是一个器具，返回的是一个对象，并不会返回张量
    batch = next(item(train_loader))  分批次取出数据

    images,labels = batch  获取图像和标签

    preds = network(images) 将图像张量传入实例化对象network，会内部调用forward方法输出预测值
    loss = F.loss(preds,labels) 计算损失值，注意返回的并不是一个单纯的值，还有表达式，所以我们可以简单的认为它返回了一个对象，因为在后续的反向传播中使用的也是该返回对象
    correct_num = get_num_correct(preds,labels)
    print("使用初始化权重求得的预测值和准确率，此处打印的loss和准确是是进行了一次前向传播计算出来的，并没有对初始化参数进行更新",loss.item(),correct_num)))

    loss.backward()  反向传播求导，会得到每一层的可更新参数的导数存在属性的weight属性下的grad属性中，注意不敢使用什么优化算法，后向传播是必须的，而torch也是单独将后向传播方法罗列出来了，并没有将其和其他的优化算法进行绑定，所以不论是使用什么优化算法，都需要进行后向传播的调用，然后再使用以该优化算法定义的优化器即可，也就是优化器的step方法
    #在使用完后向传播以后还需要进行优化，那么就要看使用什么优化算法了，普通的梯度下降就是直接使用后向传播求得的参数grad和学习率更新w和b即可，而高级的比如Adam还需要借用指数加权平均数，下面我们使用优化器进行优化
    
    optimizer.step()#运行优化器，network实例化对象中的参数已经更新
    preds = network(images)#可以再次使用更新后的参数预测同样的吐下你给
    loss = F.loss(preds,labels)
    correct_num = get_num_correct(preds,labels)
    print("运行一次完整的优化算法以后使用新的参数对同样的图像进行预测",loss,correct_num)

    以上就是一次完整的优化
    """
















































